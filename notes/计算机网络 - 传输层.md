<!-- GFM-TOC -->
* [UDP 和 TCP 的特点](#udp-和-tcp-的特点)
* [UDP 首部格式](#udp-首部格式)
* [TCP 首部格式](#tcp-首部格式)
* [TCP和UDP 数据大小](#TCP和UDP-数据部分大小)
* [TCP和UDP 的应用](#TCP和UDP-的应用)
* [TCP 的三次握手](#tcp-的三次握手)
* [TCP 的四次挥手](#tcp-的四次挥手)
* [TCP 可靠传输](#tcp-可靠传输)
* [TCP 滑动窗口](#tcp-滑动窗口)
* [TCP 流量控制](#tcp-流量控制)
* [TCP 拥塞控制](#tcp-拥塞控制)
    * [1. Reno](#1-Reno)
    * [2. Cubic](#2-Cubic)
    * [3. BBR](#3-BBR)
<!-- GFM-TOC -->


网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。

# UDP 和 TCP 的特点

- 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。

- 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。

# UDP 首部格式

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/d4c3a4a1-0846-46ec-9cc3-eaddfca71254.jpg" width="600"/> </div><br>

首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。

# TCP 首部格式

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/55dc4e84-573d-4c13-a765-52ed1dd251f9.png" width="700"/> </div><br>

-   **序号**   ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。

-   **确认号**   ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。

-   **数据偏移**   ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。

-   **确认 ACK**   ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。

-   **同步 SYN**   ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。

-   **终止 FIN**   ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。

-   **复位 RST**   ：用来异常情况的关闭连接，发送RST报文段关闭连接时，不必等缓冲区的数据都发送出去，直接丢弃缓冲区中的数据。而接收端收到RST后，也不必发送ack来确认。常见几种情况:
    - 1.连接一个未监听的端口
    - 2.目的主机或者网络路径中防火墙拦截
    - 3.socket接收缓冲取Recv-Q中的数据未完全被应用程序读取时关闭该socket
    - 4.向已关闭的socket发送数据
    - 5.向已关闭的连接发送FIN
    - 6.向已经消逝的连接中发送数据

-   **窗口**   ：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。Window Size 占两个byte，最大值为65535。如果A发送给B window size = 8192，意思是：B最多可以连续发送8192 byte 给A，在A ACK这8192 byte之前。那A的这个8192byte 怎么来的呢？ 一般来说，8192byte就是A的接收缓区，A_Receive_Buffer= 8192，如果B不小心发送超过8192 byte，如果application没有及时取走，则超过8192 byte 数据可能会因为A_Receive_Buffer满而被丢弃，所以B会严格遵守A的 advertised window size，如果A通告的window = 0，则B一定不会发送数据。

# TCP和UDP 数据部分大小

UDP 头部的`UDP长度`是两个字节，范围是0-65535  
IP 头部的`总长度`也是两个字节，范围是0-65535

避免分片(MTU为1500):  
- UDP 包的数据部分大小就应该是 1500 - IP头(20) - UDP头(8) = 1472(Bytes)  
- TCP 包的数据部分大小就应该是 1500 - IP头(20) - TCP头(20) = 1460 (Bytes)  

在应用层，你的Data最大长度为1472。当我们的UDP包中的数据多于MTU(1472)时，发送方的IP层需要分片fragmentation进行传输，而在接收方IP层则需要进行数据报重组，由于UDP是不可靠的传输协议，如果分片丢失导致重组失败，将导致UDP数据包被丢弃。在普通的局域网环境下，UDP的数据最大为1472字节最好(避免分片重组)。   

但在网络编程中，Internet中的路由器可能有设置成不同的值(小于默认值)，Internet上的标准MTU值为576，所以Internet的UDP编程时数据长度最好在576－20－8＝548字节以内


实际应用:
 - 用 UDP 协议发送时，用sendto函数最大能发送数据的长度为：65535- IP头(20) - UDP头(8)＝65507字节
 -  用 TCP 协议发送时，由于 TCP 是数据流协议，因此不存在包大小的限制（暂不考虑缓冲区的大小），这是指在用send函数时，数据长度参数不受限制。而实际上，所指定的这段数据并不一定会一次性发送出去，如果这段数据比较长，会被分段发送，如果比较短，可能会等待和下一次数据一起发送。

 # TCP和UDP 的应用

UDP的三大使用场景:
- 需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用
- 不需要一对一沟通，建立连接，而是可以广播的应用
- 需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞也要发出去

UDP的使用例子: 

### 一.网页或者APP的访问 

原来访问网页和手机APP都是基于HTTP协议的。HTTP协议是基于TCP的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP可能还会断了重连，也是很耗时的。而且目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。

而QUIC（全称Quick UDP Internet Connections，快速UDP互联网连接）是Google提出的一种基于UDP改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。

QUIC在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制。

### 二.流媒体的协议  

现在直播协议多使用基于TCP的RTMP。TCP的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，老的视频帧丢了其实也就丢了，实时性比较重要，宁可丢包，也不要卡顿的。

另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。

还有就是当网络不好的时候，TCP协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于UDP实现了自己的视频传输协议。

### 三.实时游戏  

游戏有一个特点，就是实时性比较高。因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护TCP连接需要在内核维护一些数据结构，因而一台机器能够支撑的TCP连接数目是有限的，然后UDP由于是没有连接的，在异步IO机制引入之前，常常是应对海量客户端连接的策略。

另外还是TCP的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。

如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡1秒，等能动了都已经死了。

游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。

### 四.IoT物联网   

一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护TCP协议代价太大；另一方面，物联网对实时性要求也很高，而TCP还是因为上面的那些原因导致时延大。Google旗下的Nest建立Thread Group，推出了物联网通信协议Thread，就是基于UDP协议的。

### 五.移动通信领域  

在4G网络里，移动流量上网的数据面对的协议GTP-U是基于UDP的。因为移动网络协议比较复杂，而GTP协议本身就包含复杂的手机上线下线的通信协议。如果基于TCP，TCP的机制就显得非常多余。


### 应用

#### 基于TCP协议的应用：

HTTP协议：超文本传输协议，用于普通浏览

HTTPS协议：安全超文本传输协议，身披SSL外衣的HTTP协议

FTP协议：文件传输协议，用于文件传输

POP3协议：邮局协议，收邮件使用

SMTP协议：简单邮件传输协议，用来发送电子邮件

Telent协议：远程登陆协议，通过一个终端登陆到网络

SSH协议：安全外壳协议，用于加密安全登陆，替代安全性差的Telent协议


#### 基于UDP协议的应用：

DNS、DHCP、QUIC、VXLAN、NTP、BOOTP

# TCP 的三次握手

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/e92d0ebc-7d46-413b-aec1-34a39602f787.png" width="600"/> </div><br>

假设 A 为客户端，B 为服务器端。

- 首先 B 处于 LISTEN（监听）状态，等待客户端的连接到来。

- A 向 B 发送连接请求报文段，SYN=1，ACK=0，选择一个初始的序号 x。

- B 收到连接请求报文段，如果同意建立连接，则向 A 发送连接确认报文，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。

- A 收到 B 的连接确认报文后，还要向 B 发出确认报文，确认号为 y+1，序号为 x+1。

- B 收到 A 的确认后报文，连接建立。

**三次握手的原因**  

第三次握手是为了防止失效的连接请求报文段到达服务器，让服务器错误打开连接。

客户端发送的连接请求报文如果在网络中滞留，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新发送请求报文。此时这个滞留的连接请求报文段在消亡之前还是到达了服务器，如果只有两次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求报文的连接确认，不进行第三次握手，因此就不会再次打开连接。

# TCP 的四次挥手

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/f87afe72-c2df-4c12-ac03-9b8d581a8af8.jpg" width="600"/> </div><br>

- A 发送 FIN 报文段给 B，A 表示不再发送数据。

- B 收到后返回一个ACK报文段给 A，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。

- 当 B 不再需要连接时，发送 FIN+ACK 报文段给A。

- A 收到后发出 ACK 报文段，进入 TIME-WAIT 状态，等待 2 MSL（最大报文存活时间）后进入 CLOSED 状态。

- B 收到 A 的 ACK 报文段后进入 CLOSED 状态。

**四次挥手的原因**  

TCP是全双工的，每个方向都要进行单独关闭。当一方完成数据发送任务后，发送一个FIN报文来终止这一方向的连接，这意味着不再发送数据，但是还可以接收数据，除非对方也发送了FIN报文。假设client端是主动发起方，当server端收到Client端的FIN报文后，知道它不再发送数据过来了。但server端自己还有数据没发完，不想立即关闭连接，所以只能先回复一个ACK报文，告诉client端，"你发的FIN报文我收到了"。server端等所有的数据都发送完了，然后发一个FIN+ACK报文给client端，client端收到后发送一个ACK报文给server端，server端收到了，然后才真正关闭了连接，故需要四步握手。

**FIN_WAIT_2**  

如果A在这个状态，B无法回应A，TCP协议里面并没有对这个状态的处理，但是Linux有，可以调整`tcp_fin_timeout`这个参数，设置一个超时时间，默认是60s。对于TCP连接的多个状态的处理，Linux内核都有对应的tcp参数。

**TIME_WAIT**  

客户端接收到服务器端的 FIN 报文段后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL(Linux是60s)。这么做有两个理由：

- 确保 A 最后发送的的 ACK 报文段能够到达 B。如果处于 LAST-ACK 状态的 B 没收到 A 发送来的ACK报文段，那么就会重新发送 FIN+ACK 报文段。接着 A 重传一次确认，重新启动 2MSL 计时器。最后 A 和 B 都进入 CLOSED 状态。如果 A 不等待一段时间，就收不到 B 重传的 FIN+ACK 报文段，也就不会重新发送一次确认，这样B无法正常进入 CLOSED 状态  
(注意：B 超过 2MSL 还没收到 ACK 报文段，B重发 FIN+ACK 报文段，A返回一个 RST)。

- 等待一段时间是为了让本连接持续时间内所产生的所有报文段都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文段。

# TCP 可靠传输

TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。

一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下：

<div align="center"><img src="https://latex.codecogs.com/gif.latex?RTTs=(1-a)*(RTTs)+a*RTT" class="mathjax-pic"/></div> <br>
其中，0 ≤ a ＜ 1，RTTs 随着 a 的增加更容易受到 RTT 的影响。

超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下：

<div align="center"><img src="https://latex.codecogs.com/gif.latex?RTO=RTTs+4*RTT_d" class="mathjax-pic"/></div> <br>
其中 RTT<sub>d</sub> 为偏差的加权平均值。

# TCP 滑动窗口

窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。

发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。

接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/a3253deb-8d21-40a1-aae4-7d178e4aa319.jpg" width="800"/> </div><br>

# TCP 流量控制

流量控制是为了控制发送方发送速率，保证接收方来得及接收。

接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。

**注意**：发送端收到零窗口的确认时，就启动 `persistent timer` 这个计时器，当到期时，发送端TCP就发送一个特殊的报文段，叫探测报文段，探测报文段提醒接收端TCP，确认已丢失，必须重传。

# TCP 拥塞控制

 TCP通过维护一个拥塞窗口来进行拥塞控制，拥塞控制的原则是，只要网络中没有出现拥塞，拥塞窗口的值就可以再增大一些，以便把更多的数据包发送出去，但只要网络出现拥塞，拥塞窗口的值就应该减小一些，以减少注入到网络中的数据包数。

TCP拥塞控制算法发展的过程中出现了如下几种不同的思路：

- 基于丢包的拥塞控制：将丢包视为出现拥塞，采取缓慢探测的方式，逐渐增大拥塞窗口，当出现丢包时，将拥塞窗口减小，如Reno、Cubic等。
- 基于时延的拥塞控制：将时延增加视为出现拥塞，延时增加时增大拥塞窗口，延时减小时减小拥塞窗口，如Vegas、FastTCP等。
- 基于链路容量的拥塞控制：实时测量网络带宽和时延，认为网络上报文总量大于带宽时延乘积时出现了拥塞，如BBR。
- 基于学习的拥塞控制：没有特定的拥塞信号，而是借助评价函数，基于训练数据，使用机器学习的方法形成一个控制策略，如Remy。
拥塞控制算法的核心是选择一个有效的策略来控制拥塞窗口的变化，下面介绍几种经典的拥塞控制算法。


<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/51e2ed95-65b8-4ae9-8af3-65602d452a25.jpg" width="500"/> </div><br>

## 1. Reno
 
发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。

为了便于讨论，做如下假设：

- 接收方有足够大的接收缓存，因此不会发生流量控制；
- 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。


<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/910f613f-514f-4534-87dd-9b4699d59d31.png" width="800"/> </div><br>

`慢开始与拥塞避免`

发送的最初执行慢开始，令 cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 ...

注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd >= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。

如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始。

`快重传与快恢复`

在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M<sub>1</sub> 和 M<sub>2</sub>，此时收到 M<sub>4</sub>，应当发送对 M<sub>2</sub> 的确认。

在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M<sub>2</sub>，则 M<sub>3</sub> 丢失，立即重传 M<sub>3</sub>。

在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。

慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。

适用场景：适用于低延时、低带宽的网络。


<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/f61b5419-c94a-4df1-8d4d-aed9ae8cc6d5.png" width="600"/> </div><br>


## 2. Cubic

Cubic 是Linux内核2.6.18开始的默认TCP拥塞控制算法(可以看作Reno的高级版本)

适用场景：适用于高带宽、低丢包率网络，能够有效利用带宽。


## 3. BBR

<div align="center"> <img src="https://pic3.zhimg.com/80/v2-87e9aa069720faf6bf16b8d10a72265e_hd.jpg" width="600"/> </div><br>


BBR 是谷歌在2016年提出的一种新的拥塞控制算法，已经在Youtube服务器和谷歌跨数据中心广域网上部署，据Youtube官方数据称，部署BBR后，在全球范围内访问Youtube的延迟降低了53%，在时延较高的发展中国家，延迟降低了80%。目前BBR已经集成到Linux 4.9以上版本的内核中。

BBR算法不将出现丢包或时延增加作为拥塞的信号，而是认为当网络上的数据包总量大于瓶颈链路带宽和时延的乘积时才出现了拥塞，所以BBR也称为基于拥塞的拥塞控制算法（Congestion-Based Congestion Control）。BBR算法周期性地探测网络的容量，交替测量一段时间内的带宽极大值和时延极小值，将其乘积作为作为拥塞窗口大小（交替测量的原因是极大带宽和极小时延不可能同时得到，带宽极大时网络被填满造成排队，时延必然极大，时延极小时需要数据包不被排队直接转发，带宽必然极小），使得拥塞窗口始的值始终与网络的容量保持一致。

由于BBR的拥塞窗口是精确测量出来的，不会无限的增加拥塞窗口，也就不会将网络设备的缓冲区填满，避免了出现Bufferbloat问题，使得时延大大降低。如图4所示，网络缓冲区被填满时时延为250ms，Cubic算法会继续增加拥塞窗口，使得时延持续增加到500ms并出现丢包，整个过程Cubic一直处于高时延状态，而BBR由于不会填满网络缓冲区，时延一直处于较低状态。

linux开启bbr 
```shell
# 通过uname -r命令检查内核版本是否大于4.9
uname -r

# 启用BBR拥塞算法
# 加载内核模块
modprobe tcp_bbr
echo "tcp_bbr" >> /etc/modules-load.d/modules.conf

# 修改内核参数
echo "net.core.default_qdisc=fq" >> /etc/sysctl.conf
echo "net.ipv4.tcp_congestion_control=bbr" >> /etc/sysctl.conf
sysctl -p

# 检查BBR是否已经开启
# 如结果都有bbr，则表示已经开启BBR算法
lsmod | grep bbr
sysctl net.ipv4.tcp_available_congestion_control
sysctl net.ipv4.tcp_congestion_control

```

适用场景：适用于高带宽、高时延、有一定丢包率的长肥网络，可以有效降低传输时延，并保证较高的吞吐量。
